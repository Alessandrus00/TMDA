{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ed58ea09",
   "metadata": {},
   "source": [
    "<h1 align=\"center\">TMDA (Transportation Mode Detector and Analyzer)</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "200b16ca",
   "metadata": {},
   "source": [
    "<h2 align=\"center\">Introduction</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a568ac0",
   "metadata": {},
   "source": [
    "Transportation mode detection (TMD) is a well-known sub-task of a more general one called Human Activity Recognition (HAR), which aim to understand what activities a user is performing, through data produced during those activities.\n",
    "\n",
    "For this particular project only the classes bus, car, still, train and walking are considered to be a transportation mode, while data used to train a classifier comes from accelerometer and gyroscope sensors, read with a frequency of 20 Hz.\n",
    "\n",
    "To show more about a detection, location data (latitude and longitude) are also included, but not used in the training phase."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8b49510",
   "metadata": {},
   "source": [
    "<h2 align=\"center\">Workflow</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72988187",
   "metadata": {},
   "source": [
    "<img src=\"./workflow/WorkFlow.drawio.png\" width=\"800\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57e076db",
   "metadata": {},
   "source": [
    "<h2 align=\"center\">Data Acquisition</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cfacff9",
   "metadata": {},
   "source": [
    "<img src=\"./img/phyphox.png\" width=\"100\" />\n",
    "\n",
    "Data are collected via the mobile app **Phyphox**, that allows to select sensors to read from and the reading frequency (in this case 20 Hz).\n",
    "\n",
    "Once the recording is started, sensors data are read and, at the end of it, can be exported as a zip file, containing one CSV for each sensors, plus metadata (such as time of recording and device used).\n",
    "\n",
    "A script in python will listen to a folder containing the above zip files and, when there is something to read, it will extract and merge sensors CSVs into one single CSV called **sensors.csv**.\n",
    "\n",
    "In **update_sensors_csv()** is also performed a filtering of sensors data older than 5 seconds from the start of the recording, so that only the first 5 seconds of reading for each sensors are kept.\n",
    "\n",
    "In future this could be an embedded function of the recording app. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed80ce6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "while(True):\n",
    "        # get a list of zip files\n",
    "        zip_list = glob.glob('*.zip') \n",
    "        # if is not empty, update sensors.csv, else sleep for 5 sec\n",
    "        if len(zip_list) > 0:\n",
    "            zf = get_earliest_zipfile(zip_list)\n",
    "            update_sensors_csv(zf)\n",
    "        sleep(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a61cf6bb",
   "metadata": {},
   "source": [
    "<h2 align=\"center\"> Data Ingestion</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6251dbdb",
   "metadata": {},
   "source": [
    "<img src=\"./img/logstash_logo.svg\" width=\"100\" />\n",
    "\n",
    "Ingestion is provided by Logstash, that reads updated lines of the file sensors.csv constantly, sending them to a kafka topic called **sensors-raw**.\n",
    "\n",
    "That's it, thank you Logstash <3."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "441ee29d",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "  <img src=\"./memes/logstash_heaven.jpg\"/>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6310e3ff",
   "metadata": {},
   "source": [
    "<h2 align=\"center\"> Stream Processing</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f985cacf",
   "metadata": {},
   "source": [
    "<img src=\"./img/stream_processing.png\" width=\"250\" />\n",
    "\n",
    "A Kafka cluster is responsible for handling streams coming from Logstash and storing them efficientely to specific topics.\n",
    "\n",
    "Data are also replicated across different Kafka brokers, so if one of them goes down (temporarily or permanently) we don't lose any data.\n",
    "\n",
    "There are two main topics: sensors-raw (for data coming from Logstash) and sensors (for data cleaned by a Spark container) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c105db27",
   "metadata": {},
   "source": [
    "<h2 align=\"center\">Data Cleaning</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "360a7beb",
   "metadata": {},
   "source": [
    "<img src=\"./img/spark_logo.png\" width=\"150\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01f9acc4",
   "metadata": {},
   "source": [
    "Before making any prediction, data must be cleaned, in order to extract features like **mean**, **min**, **max** and **stddev** in a 5 seconds time window.\n",
    "\n",
    "For this reason session windows were used."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb609cf4",
   "metadata": {},
   "source": [
    "<img src=\"./img/session_windows.png\" width=\"800\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc516e78",
   "metadata": {},
   "source": [
    "A session window begins with a single data point and broadens itself in case of the upcoming element has been collected inside of the gap period.\n",
    "When the last item is accepted, the session window ends when no items are acknowledged inside of the gap period.\n",
    "\n",
    "Note that we can't control the dimension of a window, wich is determined by events themselves."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2461edfe",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "  <img src=\"./memes/session_window_5sec.jpg\"/>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ada0b4ab",
   "metadata": {},
   "source": [
    "Remember, in data acquisition a function that takes only 5 seconds of data for each sensor was already applied"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bee282f",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "  <img src=\"./memes/normal_windows_why.jpg\"/>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41760acf",
   "metadata": {},
   "source": [
    "Because if user1 records at time **t** and user2 at time **t+1**, tumbling window is in \\[**t**, **t+5**\\].\n",
    "So at time **t+5** we do grouping without including the last second of user2 in this window.\n",
    "At the end we grouped 5 seconds for user1 and 4 seconds for user2.\n",
    "\n",
    "<u>We need to use session windows to have dedicated windows for each user</u>, beginning when events for that particular user occour."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53e24793",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "  <img src=\"./memes/confusion.jpg\"/>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f02fe6e5",
   "metadata": {},
   "source": [
    "If you didn't understand, don't worry and take a look at this https://towardsdatascience.com/spark-3-2-session-windowing-feature-for-streaming-data-e404d92e267"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2717e1d",
   "metadata": {},
   "source": [
    "<h2 align=\"center\">Value Extraction</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60f2db1c",
   "metadata": {},
   "source": [
    "<img src=\"./img/spark_logo.png\" width=\"150\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaf6be54",
   "metadata": {},
   "source": [
    "Using the model created at the end of the training phase, we enrich our data with predictions on transportation mode, given a set of input features (extracted earlier)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85d7f3fa",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "  <img src=\"./memes/tuning.jpg\"/>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25b046b7",
   "metadata": {},
   "source": [
    "<h2 align=\"center\"> Data Indexing</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9f38b94",
   "metadata": {},
   "source": [
    "<img src=\"./img/elastic_search_logo.svg\" width=\"80\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0a94ba5",
   "metadata": {},
   "source": [
    "Data indexing allow to search and query our data at the speed of light, because: \"Elasticsearch is fast. Really, really fast.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77f43106",
   "metadata": {},
   "source": [
    "<h2 align=\"center\"> Data Visualization</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1612f867",
   "metadata": {},
   "source": [
    "<img src=\"./img/kibana_logo.svg\" width=\"80\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1bbb3fe",
   "metadata": {},
   "source": [
    "Finally we can visualize the results of computed on streaming data through charts realized with Kibana"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21e8c9ef",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "  <img src=\"./img/charts.jpg\"/>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9064520d",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "  <img src=\"./memes/we_did_it.gif\"/>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a100e7e",
   "metadata": {},
   "source": [
    "# Thanks"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
