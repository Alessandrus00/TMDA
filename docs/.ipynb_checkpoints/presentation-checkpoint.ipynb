{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d6c0ed27",
   "metadata": {},
   "source": [
    "# TMDA (Transportation Mode Detector and Analyzer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df6fb1a7",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Transportation mode detection (TMD) is a well-known sub-task of a more general one called Human Activity Recognition (HAR), which aim to understand what activities a user is performing, through data produced during those activities.\n",
    "\n",
    "For this particular project only the classes bus, car, still, train and walking are considered to be a transportation mode, while data used to train a classifier comes from accelerometer and gyroscope sensors, read with a frequency of 20 Hz.\n",
    "\n",
    "To show more about a detection, location data (latitude and longitude) are also included, but not used in the training phase."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2d34751",
   "metadata": {},
   "source": [
    "## Workflow\n",
    "\n",
    "<img src=\"./workflow/WorkFlow.drawio.png\" width=\"800\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5617bfcf",
   "metadata": {},
   "source": [
    "## Data Acquisition\n",
    "\n",
    "Data are collected via the mobile app **Phyphox**, that allows to select sensors to read from and the reading frequency (in this case 20 Hz).\n",
    "\n",
    "Once the recording is started, sensors data are read and, at the end of it, can be exported as a zip file, containing one CSV for each sensors, plus metadata (such as time of recording and device used).\n",
    "\n",
    "A script in python will listen to a folder containing the above zip files and, when there is something to read, it will extract and merge sensors CSVs into one single CSV called **sensors.csv**.\n",
    "\n",
    "In **update_sensors_csv()** is also performed a filtering of sensors data older than 5 seconds from the start of the recording, so in the end only the first 5 seconds of reading are kept.\n",
    "\n",
    "In future this coul be an embedded function of the recording app. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54017fe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "while(True):\n",
    "        # get a list of zip files\n",
    "        zip_list = glob.glob('*.zip') \n",
    "        # if is not empty, update sensors.csv, else sleep for 5 sec\n",
    "        if len(zip_list) > 0:\n",
    "            zf = get_earliest_zipfile(zip_list)\n",
    "            update_sensors_csv(zf)\n",
    "        sleep(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fce94798",
   "metadata": {},
   "source": [
    "## Data Ingestion\n",
    "\n",
    "Ingestion is provided by Logstash, that reads updated lines of the file sensors.csv constantly, sending them to a kafka topic called **sensors-raw**.\n",
    "\n",
    "That's it, thank you Logstash <3."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c087c38",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "  <img src=\"./memes/logstash_heaven.jpg\"/>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3a8bb3e",
   "metadata": {},
   "source": [
    "## Stream Processing\n",
    "\n",
    "A Kafka cluster is responsible for handling streams coming from Logstash and storing them efficientely to specific topics.\n",
    "\n",
    "Data are also replicated across different Kafka brokers, so if one of them goes down (temporarily or permanently) we don't lose any data.\n",
    "\n",
    "There are two main topics: sensors-raw (for data coming from Logstash) and sensors (for data cleaned by a Spark container) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3894b11d",
   "metadata": {},
   "source": [
    "## Data Cleaning\n",
    "\n",
    "Before making any prediction, data must be cleaned, in order to extract features like **mean**, **min**, **max** and **stddev** in a 5 seconds time window.\n",
    "\n",
    "For this reason session windows were used.\n",
    "\n",
    "A session window begins with a single data point and broadens itself in case of the upcoming element has been collected inside of the gap period.\n",
    "When the last item is accepted, the session window ends when no items are acknowledged inside of the gap period.\n",
    "\n",
    "Note that we can't control the dimension of a window, wich is determined by events themselves"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
