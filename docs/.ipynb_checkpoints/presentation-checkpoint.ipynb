{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ed58ea09",
   "metadata": {},
   "source": [
    "<h1 align=\"center\">TMDA (Transportation Mode Detector and Analyzer)</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "200b16ca",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a568ac0",
   "metadata": {},
   "source": [
    "Transportation mode detection (**TMD**) is a well-known sub-task of a more general one called Human Activity Recognition (**HAR**), which aim to understand what activities a user is performing, through data produced during those activities.\n",
    "\n",
    "In TMD, users activities are specific commuting actions, meaning that one can get from point **A** to point **B** using several transportation modes (one at time). Like HAR, activity detection run on data, that in this case needs to be produced during commuting actions.\n",
    "\n",
    "From the above definition, two main questions arise:\n",
    "* Which **transportation modes** are going to be considered?\n",
    "* What kind of **data** can be used to achieve the goal? \n",
    "* **How to use data** to detect the transportation mode?\n",
    "\n",
    "In this project, transportation modes are restricted to **bus, car, still, train** and **walking**, while data comes from user's smartphone **accelerometer** and **gyroscope** sensors, which are two very discriminative sensors for this task. Reading frequency is set to **20 Hz**, so in a **5** seconds window **100** readings are performed, which contain a sufficient amount of data to describe how a user is moving (after a cleaning process). More info can be found in this UNIBO <a href=\"http://cs.unibo.it/projects/us-tm2017/index.html\">study</a>.\n",
    " \n",
    "To show more about a detection, location data (**latitude** and **longitude**) are also included, but not used as estimation features.\n",
    "\n",
    "For the last question, only one word can answer: **\"Machine Learning\"**. Using labeled data about sensors (with the transportation mode), a model can be trained to classify unseen unlabeled data.\n",
    "\n",
    "**Magic? No, Math!**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52a8d7f0",
   "metadata": {},
   "source": [
    "## Motivations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4286ed2f",
   "metadata": {},
   "source": [
    "TMD can be used, for example, by public transport companies (such as FCE) to better understand how people move everyday, and improve their services if public transports are poorly used.\n",
    "\n",
    "For example, if bus are the less used among the others, a transport company can increse the quality of the service by increasing the number of run, covering a much greater area of a city and so on.\n",
    "\n",
    "This can potentially have a strong impact on the environment, reducing **CO2 emissions** and **pollution**, making a city more green."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8b49510",
   "metadata": {},
   "source": [
    "## Workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72988187",
   "metadata": {},
   "source": [
    "<img src=\"./workflow/WorkFlow.drawio.png\" width=\"800\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57e076db",
   "metadata": {},
   "source": [
    "## Data Acquisition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48ae1a6d",
   "metadata": {},
   "source": [
    "<img src=\"./img/phyphox.png\" width=\"100\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cfacff9",
   "metadata": {},
   "source": [
    "Data are collected via the mobile app **Phyphox**, that allows to select sensors to read from and reading frequency (in this case 20 Hz).\n",
    "\n",
    "Once the recording is started, sensors data are read and, at the end of it, can be exported as a zip file, containing one CSV for each sensor, plus metadata (such as timestamp of recording and device used).\n",
    "\n",
    "A script in python will listen to a folder containing the above zip files and, when there is something to read, it will extract and merge sensors CSVs into one single CSV called **sensors.csv**.\n",
    "\n",
    "In **update_sensors_csv()** is also performed a filtering of sensors data older than 5 seconds from the start of the recording, so that for each sensor only the first 5 seconds of readings are kept.\n",
    "\n",
    "Data stored in sensors.csv are **raw** and will be cleaned later by a separated script running in a container."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed80ce6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "while(True):\n",
    "        # get a list of zip files\n",
    "        zip_list = glob.glob('*.zip') \n",
    "        # if is not empty, update sensors.csv, else sleep for 5 sec\n",
    "        if len(zip_list) > 0:\n",
    "            zf = get_earliest_zipfile(zip_list)\n",
    "            update_sensors_csv(zf)\n",
    "        sleep(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d66f8ae",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13164849",
   "metadata": {},
   "source": [
    "<img src=\"./img/spark_logo.png\" width=\"150\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3162a40f",
   "metadata": {},
   "source": [
    "<i>\"Apache Spark is a unified engine for large-scale data analytics, used to execute data engineering, data science, and machine learning on single-node machines or clusters.\"</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c47e39b8",
   "metadata": {},
   "source": [
    "A **Gradient-Boosted Tree** classification model is trained on dataset containing **features** (clean sensors) + **target** (transportation mode) using **Spark MLlib**, which is a scalable machine learning library that comes with Spark. Resulting model is then saved to disk for later use.\n",
    "\n",
    "At the end of the training phase, the model accuracy was **72%**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83d2a991",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "  <img src=\"./memes/nice.jpg\"/>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a61cf6bb",
   "metadata": {},
   "source": [
    "## Data Ingestion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86e8f91a",
   "metadata": {},
   "source": [
    "<img src=\"./img/logstash_logo.svg\" width=\"100\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f12f13f4",
   "metadata": {},
   "source": [
    "<i>\"Logstash is a free and open server-side data processing pipeline that ingests data from a multitude of sources, transforms it, and then sends it to your favorite \"stash.\"\"</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6251dbdb",
   "metadata": {},
   "source": [
    "Data ingestion is provided by Logstash, that reads updated lines of the file **sensors.csv** and send them to a kafka topic called **sensors-raw**.\n",
    "\n",
    "That's it, thank you Logstash <3."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "441ee29d",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "  <img src=\"./memes/logstash_heaven.jpg\"/>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6310e3ff",
   "metadata": {},
   "source": [
    "## Stream Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2098150",
   "metadata": {},
   "source": [
    "<img src=\"./img/stream_processing.png\" width=\"250\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba90ee41",
   "metadata": {},
   "source": [
    "<i>\"Apache Kafka is an open-source distributed event streaming platform used by thousands of companies for high-performance data pipelines, streaming analytics, data integration, and mission-critical applications.\"</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f985cacf",
   "metadata": {},
   "source": [
    "A Kafka cluster is responsible for handling streams coming from Logstash and storing them efficientely to specific topics.\n",
    "\n",
    "Data are also replicated across different Kafka brokers, so if one of them goes down (temporarily or permanently) we don't lose any data.\n",
    "\n",
    "There are two main topics: **sensors-raw** (for data coming from Logstash) and **sensors** (for data cleaned by a container running Spark) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c105db27",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "360a7beb",
   "metadata": {},
   "source": [
    "<img src=\"./img/spark_logo.png\" width=\"150\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01f9acc4",
   "metadata": {},
   "source": [
    "Before making any prediction, data must be cleaned, in order to extract the following features: \n",
    "* **mean**\n",
    "* **min** \n",
    "* **max**\n",
    "* **stddev**\n",
    "\n",
    "in a 5 seconds time window. Here is the main function of data cleaning task:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e06656f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(df):\n",
    "    df = df \\\n",
    "        .withColumn('acc', compute_magnitude(col('acc_x'), col('acc_y'), col('acc_z'))) \\\n",
    "        .withColumn('gyro', compute_magnitude(col('gyro_x'), col('gyro_y'), col('gyro_z'))) \\\n",
    "        .withWatermark('timestamp', '2 seconds') \\\n",
    "        .groupBy(\n",
    "            # after 2 sec of lacking data from a user we end the session\n",
    "            session_window('timestamp', '2 seconds'),\n",
    "            'user_id') \\\n",
    "        .agg(mean('acc').alias('acc_mean'),\n",
    "            min('acc').alias('acc_min'),\n",
    "            max('acc').alias('acc_max'),\n",
    "            stddev('acc').alias('acc_stddev'),\n",
    "            mean('gyro').alias('gyro_mean'),\n",
    "            min('gyro').alias('gyro_min'),\n",
    "            max('gyro').alias('gyro_max'),\n",
    "            stddev('gyro').alias('gyro_stddev'),\n",
    "            first('latitude').alias('latitude'),\n",
    "            first('longitude').alias('longitude')) \\\n",
    "        .withColumn('timestamp', col('session_window').getField('start')) \\\n",
    "        .na.fill(value=0)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5faee4e",
   "metadata": {},
   "source": [
    "A **session window** has been used, but what is it?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dad7ced0",
   "metadata": {},
   "source": [
    "### session window"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb609cf4",
   "metadata": {},
   "source": [
    "<img src=\"./img/session_windows.png\" width=\"800\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc516e78",
   "metadata": {},
   "source": [
    "<i>\"A session window begins with a single data point and broadens itself in case of the upcoming element has been collected inside of the gap period.\n",
    "When the last item is accepted, the session window ends when no items are acknowledged inside of the gap period.\"</i>\n",
    "\n",
    "Note that the window's dimension cannot be controlled, it is determined by events themselves."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2461edfe",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "  <img src=\"./memes/session_window_5sec.jpg\"/>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ada0b4ab",
   "metadata": {},
   "source": [
    "Remember, in data acquisition phase, a function that takes only 5 seconds of data for each sensor was already applied."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bee282f",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "  <img src=\"./memes/normal_windows_why.jpg\"/>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41760acf",
   "metadata": {},
   "source": [
    "It will be explained with a very simple example:\n",
    "\n",
    "If **user1** records at time **t** and **user2** at time **t+1**, a tumbling window is in the interval <b>[t, t+5]</b>.\n",
    "So at time **t+5** we do grouping without including the last second of user2 in this window.\n",
    "At the end we grouped 5 seconds for user1 and 4 seconds for user2.\n",
    "\n",
    "<u>We need to use session windows to have dedicated windows for each user</u>, beginning when events for that particular user occour."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53e24793",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "  <img src=\"./memes/confusion.jpg\"/>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f02fe6e5",
   "metadata": {},
   "source": [
    "### Still not clear?\n",
    "\n",
    "Don't worry and take a look at <a href=\"https://towardsdatascience.com/spark-3-2-session-windowing-feature-for-streaming-data-e404d92e267\">this</a>."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2717e1d",
   "metadata": {},
   "source": [
    "## Value Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60f2db1c",
   "metadata": {},
   "source": [
    "<img src=\"./img/spark_logo.png\" width=\"150\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaf6be54",
   "metadata": {},
   "source": [
    "Using the model created at the end of the training phase and a set of input features (extracted earlier), data is enriched with predictions on transportation mode.\n",
    "\n",
    "Once the predictions are made, we send the results to an Elasticsearch index called **sensors**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42239311",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "  <img src=\"./memes/still_train.jpg\"/>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25b046b7",
   "metadata": {},
   "source": [
    "## Data Indexing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9f38b94",
   "metadata": {},
   "source": [
    "<img src=\"./img/elastic_search_logo.svg\" width=\"80\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0a94ba5",
   "metadata": {},
   "source": [
    "<i>\"Elasticsearch is a distributed, free and open search and analytics engine for all types of data, including textual, numerical, geospatial, structured, and unstructured. Known for its simple REST APIs, distributed nature, speed, and scalability, Elasticsearch is the central component of the Elastic Stack\".</i>\n",
    "\n",
    "Remember: **\"Elasticsearch is fast. Really, really fast\".**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "970a5cea",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "  <img src=\"./memes/i_am_speed.jpg\"/>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77f43106",
   "metadata": {},
   "source": [
    "## Data Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1612f867",
   "metadata": {},
   "source": [
    "<img src=\"./img/kibana_logo.svg\" width=\"80\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1bbb3fe",
   "metadata": {},
   "source": [
    "<i>\"Kibana is an free and open frontend application that sits on top of the Elastic Stack, providing search and data visualization capabilities for data indexed in Elasticsearch. Commonly known as the charting tool for the Elastic Stack\".</i>\n",
    "\n",
    "Finally we can visualize the results through charts realized with Kibana:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21e8c9ef",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "  <img src=\"./img/kibana_dashboard.png\"/>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a100e7e",
   "metadata": {},
   "source": [
    "## Thanks for your attention!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9064520d",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "  <img src=\"./memes/we_did_it.gif\"/>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b4e67ba",
   "metadata": {},
   "source": [
    "## Credits\n",
    "\n",
    "Alessandro Resta"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
